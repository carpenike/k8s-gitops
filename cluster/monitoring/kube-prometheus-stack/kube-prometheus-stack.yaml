---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 5m
  chart:
    spec:
      # renovate: registryUrl=https://prometheus-community.github.io/helm-charts
      chart: kube-prometheus-stack
      version: 14.0.1
      sourceRef:
        kind: HelmRepository
        name: prometheus-community-charts
        namespace: flux-system
      interval: 5m
  values:
    # defaultRules:
    #   rules:
    #     kubeApiserverAvailability: false
    #     kubeApiserver: false
    server:
      resources:
        requests:
          memory: 1500Mi
          cpu: 25m
        limits:
          memory: 2000Mi
    prometheusOperator:
      createCustomResource: false
      prometheusConfigReloaderImage:
        repository: quay.io/prometheus-operator/prometheus-config-reloader
        tag: v0.46.0
      configmapReloadImage:
        repository: docker.io/jimmidyson/configmap-reload
        tag: v0.5.0
    alertmanager:
      alertmanagerSpec:
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: longhorn
              resources:
                requests:
                  storage: 10Gi
        tolerations:
        - key: "arm"
          operator: "Exists"
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: "nginx"
          kubernetes.io/tls-acme: "true"
          cert-manager.io/cluster-issuer: letsencrypt-prod
          nginx.ingress.kubernetes.io/auth-url: "https://auth.holthome.net/oauth2/auth"
          nginx.ingress.kubernetes.io/auth-signin: "https://auth.holthome.net/oauth2/start?rd=$escaped_request_uri"
        hosts:
        - prom-alert.holthome.net
        tls:
        - hosts:
          - prom-alert.holthome.net
          secretName: alertmanager-cert
      config:
        route:
          # group_by: ['alertname', 'job']
          # group_wait: 30s
          # group_interval: 5m
          # repeat_interval: 6h
          receiver: 'alertmanager-bot'
          # routes:
          #   - match:
          #       alertname: Watchdog
          #     receiver: 'null'
        receivers:
        - name: 'null'
        - name: 'alertmanager-bot'
          webhook_configs:
          - send_resolved: true
            url: http://alertmanager-bot:8080
      templateFiles:
        pagerduty-custom.tmpl: |-
          {{- define "pagerduty.custom.description" -}}[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if ne .CommonAnnotations.summary ""}}{{ .CommonAnnotations.summary }} {{ else if ne .CommonAnnotations.message ""}}{{ .CommonAnnotations.message }} {{ else if ne .CommonAnnotations.description ""}}{{ .CommonAnnotations.description }} {{ else }}{{ .CommonLabels.alertname }}{{ end }}{{- end -}}
    nodeExporter:
      serviceMonitor:
        relabelings:
        - action: replace
          regex: (.*)
          replacement: $1
          sourceLabels:
          - __meta_kubernetes_pod_node_name
          targetLabel: kubernetes_node
    kubelet:
      serviceMonitor:
        metricRelabelings:
        - action: replace
          sourceLabels:
          - node
          targetLabel: instance
    grafana:
      enabled: false
      # image:
      #   repository: grafana/grafana
      #   tag: 7.0.4
      # tolerations:
      # - key: "arm"
      #   operator: "Exists"
      # deploymentStrategy:
      #   type: Recreate
      # persistence:
      #   enabled: true
      #   storageClassName: longhorn
      #   size: 10Gi
      #   accessModes:
      #   - ReadWriteOnce
      # env:
      #   GF_EXPLORE_ENABLED: true
      #   GF_DISABLE_SANITIZE_HTML: true
      #   GF_PANELS_DISABLE_SANITIZE_HTML: true
      # ingress:
      #   enabled: true
      #   annotations:
      #     kubernetes.io/ingress.class: "nginx"
      #     kubernetes.io/tls-acme: "true"
      #     cert-manager.io/cluster-issuer: letsencrypt-prod
      #   hosts:
      #   - grafana.holthome.net
      #   tls:
      #   - hosts:
      #     - grafana.holthome.net
      #     secretName: grafana-cert
      # plugins:
      # - natel-discrete-panel
      # - pr0ps-trackmap-panel
      # - grafana-piechart-panel
      # - savantly-heatmap-panel
      # - grafana-clock-panel
      # - https://github.com/panodata/grafana-map-panel/releases/download/0.9.0/grafana-map-panel-0.9.0.zip;grafana-worldmap-panel-ng
      # dashboardProviders:
      #   dashboardproviders.yaml:
      #     apiVersion: 1
      #     providers:
      #     - name: "default"
      #       orgId: 1
      #       folder: ""
      #       type: file
      #       disableDeletion: false
      #       editable: true
      #       options:
      #         path: /var/lib/grafana/dashboards/default
      # dashboards:
      #   default:
      #     flux-cluster:
      #       url: https://raw.githubusercontent.com/fluxcd/flux2/v0.4.2/manifests/monitoring/grafana/dashboards/cluster.json
      #       datasource: Prometheus
      #     flux-control-plane:
      #       url: https://raw.githubusercontent.com/fluxcd/flux2/v0.4.2/manifests/monitoring/grafana/dashboards/control-plane.json
      #       datasource: Prometheus
      #     nginx-dashboard:
      #       url: https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/grafana/dashboards/nginx.json
      #       datasource: Prometheus
      #     vernemq:
      #       url: https://raw.githubusercontent.com/vernemq/vernemq/master/metrics_scripts/grafana/VerneMQ%20Node%20Metrics.json
      #       datasource: Prometheus
      #     velero:
      #       url: https://grafana.com/api/dashboards/11055/revisions/2/download
      #       datasource: Prometheus
      #     1-node-exporter:
      #       url: https://grafana.com/api/dashboards/11074/revisions/9/download
      #       datasource: Prometheus
      #     speedtest:
      #       url: https://raw.githubusercontent.com/billimek/prometheus-speedtest-exporter/master/speedtest-exporter.json
      #       datasource: Prometheus
      #     zfs:
      #       url: https://grafana.com/api/dashboards/11337/revisions/1/download
      #       datasource: Prometheus
      #     TrueNAS:
      #       url: https://grafana.com/api/dashboards/12921/revisions/1/download
      #       datasource: TrueNAS
      #     # home-assistant:
      #     #   url: https://raw.githubusercontent.com/billimek/k8s-gitops/master/monitoring/prometheus-operator/grafana-dashboards/home_assistant.json
      #     #   datasource: influxdb
      #     # ups:
      #     #   url: https://raw.githubusercontent.com/billimek/k8s-gitops/master/monitoring/prometheus-operator/grafana-dashboards/ups.json
      #     #   datasource: influxdb
      #     # netdata:
      #     #   url: https://raw.githubusercontent.com/billimek/k8s-gitops/master/monitoring/prometheus-operator/grafana-dashboards/netdata.json
      #     #   datasource: Prometheus
      # sidecar:
      #   dashboards:
      #     enabled: true
      #     searchNamespace: ALL
      #   datasources:
      #     enabled: true
      #     defaultDatasourceEnabled: false
      # additionalDataSources:
      # - name: Prometheus
      #   type: prometheus
      #   access: proxy
      #   url: http://thanos-query-http:10902/
      #   isDefault: true
      # - name: loki
      #   type: loki
      #   access: proxy
      #   url: http://loki.logs.svc.cluster.local:3100
      # - name: influxdb
      #   type: influxdb
      #   access: proxy
      #   url: http://influxdb:8086
      #   database: telegraf
      # - name: home_assistant
      #   type: influxdb
      #   access: proxy
      #   url: http://influxdb:8086
      #   database: home_assistant
      # - name: speedtests
      #   type: influxdb
      #   access: proxy
      #   url: http://influxdb:8086
      #   database: speedtests
      # - name: uptimerobot
      #   type: influxdb
      #   access: proxy
      #   url: http://influxdb:8086
      #   database: uptimerobot
      # - name: TrueNAS
      #   type: influxdb
      #   access: proxy
      #   url: http://influxdb:8086
      #   database: graphitedb
      # grafana.ini:
      #   paths:
      #     data: /var/lib/grafana/data
      #     logs: /var/log/grafana
      #     plugins: /var/lib/grafana/plugins
      #     provisioning: /etc/grafana/provisioning
      #   analytics:
      #     check_for_updates: true
      #   log:
      #     mode: console
      #   grafana_net:
      #     url: https://grafana.net
    kubeEtcd:
      enabled: false
    kubeControllerManager:
      enabled: false
      # endpoints:
      # - 10.20.10.16
    kubeScheduler:
      enabled: false
      # endpoints:
      # - 10.20.10.16
    kubeProxy:
      enabled: false
    prometheus-node-exporter:
      tolerations:
      - key: "arm"
        operator: "Exists"
      - key: "armhf"
        operator: "Exists"
    prometheus:
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: "nginx"
          kubernetes.io/tls-acme: "true"
          cert-manager.io/cluster-issuer: letsencrypt-prod
          nginx.ingress.kubernetes.io/auth-url: "https://auth.holthome.net/oauth2/auth"
          nginx.ingress.kubernetes.io/auth-signin: "https://auth.holthome.net/oauth2/start?rd=$escaped_request_uri"
        hosts:
        - prom-server.holthome.net
        tls:
        - hosts:
          - prom-server.holthome.net
          secretName: prom-cert
      prometheusSpec:
        # image:
        #   repository: quay.io/prometheus/prometheus
        #   tag: v2.20.0
        replicas: 2
        replicaExternalLabelName: "replica"
        ruleSelector: {}
        ruleNamespaceSelector: {}
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorSelector: {}
        serviceMonitorNamespaceSelector: {}
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelector: {}
        podMonitorNamespaceSelector: {}
        podMonitorSelectorNilUsesHelmValues: false
        retention: 6h
        enableAdminAPI: true
        walCompression: true
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: longhorn
              resources:
                requests:
                  storage: 10Gi
        thanos:
          image: quay.io/thanos/thanos:v0.17.2
          version: v0.17.2
          objectStorageConfig:
            name: thanos
            key: object-store.yaml
  valuesFrom:
  - kind: Secret
    name: "kube-prometheus-stack-helm-values"
    optional: false
