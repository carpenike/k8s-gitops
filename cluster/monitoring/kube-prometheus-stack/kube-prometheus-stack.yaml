---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 5m
  chart:
    spec:
      # renovate: registryUrl=https://prometheus-community.github.io/helm-charts
      chart: kube-prometheus-stack
      version: 12.10.5
      sourceRef:
        kind: HelmRepository
        name: prometheus-community-charts
        namespace: flux-system
      interval: 5m
  values:
    # defaultRules:
    #   rules:
    #     kubeApiserverAvailability: false
    #     kubeApiserver: false
    server:
      resources:
        requests:
          memory: 1500Mi
          cpu: 25m
        limits:
          memory: 2000Mi
    prometheusOperator:
      createCustomResource: true
      prometheusConfigReloaderImage:
        repository: quay.io/prometheus-operator/prometheus-config-reloader
        tag: v0.44.0
      configmapReloadImage:
        repository: docker.io/jimmidyson/configmap-reload
        tag: v0.4.0
    alertmanager:
      alertmanagerSpec:
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: longhorn
              resources:
                requests:
                  storage: 10Gi
        tolerations:
        - key: "arm"
          operator: "Exists"
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: "nginx"
          kubernetes.io/tls-acme: "true"
          cert-manager.io/cluster-issuer: letsencrypt-prod
          # nginx.ingress.kubernetes.io/auth-url: "https://auth.holthome.net/oauth2/auth"
          # nginx.ingress.kubernetes.io/auth-signin: "https://auth.holthome.net/oauth2/start?rd=$escaped_request_uri"
        hosts:
        - prom-alert.holthome.net
        tls:
        - hosts:
          - prom-alert.holthome.net
          secretName: alertmanager-cert
      config:
        route:
          # group_by: ['alertname', 'job']
          # group_wait: 30s
          # group_interval: 5m
          # repeat_interval: 6h
          receiver: 'alertmanager-bot'
          # routes:
          #   - match:
          #       alertname: Watchdog
          #     receiver: 'null'
        receivers:
        - name: 'null'
        - name: 'alertmanager-bot'
          webhook_configs:
          - send_resolved: true
            url: http://alertmanager-bot:8080
        # - name: 'slack-notifications'
        #   slack_configs:
        #   - channel: '#notifications'
        #     icon_url: https://avatars3.githubusercontent.com/u/3380462
        #     username: 'Alertmanager'
        #     send_resolved: true
        #     title: |-
        #       [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }} for {{ .CommonLabels.job }}
        #       {{- if gt (len .CommonLabels) (len .GroupLabels) -}}
        #         {{" "}}(
        #         {{- with .CommonLabels.Remove .GroupLabels.Names }}
        #           {{- range $index, $label := .SortedPairs -}}
        #             {{ if $index }}, {{ end }}
        #             {{- $label.Name }}="{{ $label.Value -}}"
        #           {{- end }}
        #         {{- end -}}
        #         )
        #       {{- end }}
        #     text: >-
        #       {{ range .Alerts -}}
        #         *Alert:* {{ .Annotations.title }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}
        #       *Message:* {{ .Annotations.message }}
        #       *Details:*
        #         {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
        #         {{ end }}
        #       {{ end }}
    nodeExporter:
      serviceMonitor:
        relabelings:
        - action: replace
          regex: (.*)
          replacement: $1
          sourceLabels:
          - __meta_kubernetes_pod_node_name
          targetLabel: kubernetes_node
    kubelet:
      serviceMonitor:
        metricRelabelings:
        - action: replace
          sourceLabels:
          - node
          targetLabel: instance
    grafana:
      # image:
      #   repository: grafana/grafana
      #   tag: 7.0.4
      # tolerations:
      # - key: "arm"
      #   operator: "Exists"
      deploymentStrategy:
        type: Recreate
      persistence:
        enabled: true
        storageClassName: longhorn
        size: 10Gi
        accessModes:
        - ReadWriteOnce
      env:
        GF_EXPLORE_ENABLED: true
        GF_DISABLE_SANITIZE_HTML: true
        GF_PANELS_DISABLE_SANITIZE_HTML: true
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: "nginx"
          kubernetes.io/tls-acme: "true"
          cert-manager.io/cluster-issuer: letsencrypt-prod
        hosts:
        - grafana.holthome.net
        tls:
        - hosts:
          - grafana.holthome.net
          secretName: grafana-cert
      plugins:
      - natel-discrete-panel
      - pr0ps-trackmap-panel
      - grafana-piechart-panel
      - savantly-heatmap-panel
      - grafana-clock-panel
      - https://github.com/panodata/grafana-map-panel/releases/download/0.9.0/grafana-map-panel-0.9.0.zip;grafana-worldmap-panel-ng
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
          - name: "default"
            orgId: 1
            folder: ""
            type: file
            disableDeletion: false
            editable: true
            options:
              path: /var/lib/grafana/dashboards/default
      dashboards:
        default:
          nginx-dashboard:
            url: https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/grafana/dashboards/nginx.json
            datasource: Prometheus
          vernemq:
            url: https://raw.githubusercontent.com/vernemq/vernemq/master/metrics_scripts/grafana/VerneMQ%20Node%20Metrics.json
            datasource: Prometheus
          flux:
            url: https://grafana.com/api/dashboards/10475/revisions/1/download
            datasource: Prometheus
          1-node-exporter:
            url: https://grafana.com/api/dashboards/11074/revisions/9/download
            datasource: Prometheus
          speedtest:
            url: https://raw.githubusercontent.com/billimek/prometheus-speedtest-exporter/master/speedtest-exporter.json
            datasource: Prometheus
          zfs:
            url: https://grafana.com/api/dashboards/11337/revisions/1/download
            datasource: Prometheus
          version-checker:
            url: https://grafana.com/api/dashboards/12833/revisions/1/download
            datasource: Prometheus
          TrueNAS:
            url: https://raw.githubusercontent.com/carpenike/k8s-gitops/master/cluster/monitoring/kube-prometheus-stack/grafana-dashboards/truenas.json
            datasource: TrueNAS
          # home-assistant:
          #   url: https://raw.githubusercontent.com/billimek/k8s-gitops/master/monitoring/prometheus-operator/grafana-dashboards/home_assistant.json
          #   datasource: influxdb
          # ups:
          #   url: https://raw.githubusercontent.com/billimek/k8s-gitops/master/monitoring/prometheus-operator/grafana-dashboards/ups.json
          #   datasource: influxdb
          # netdata:
          #   url: https://raw.githubusercontent.com/billimek/k8s-gitops/master/monitoring/prometheus-operator/grafana-dashboards/netdata.json
          #   datasource: Prometheus
      sidecar:
        dashboards:
          enabled: true
          searchNamespace: ALL
        datasources:
          enabled: true
          defaultDatasourceEnabled: false
      additionalDataSources:
      - name: Prometheus
        type: prometheus
        access: proxy
        url: http://thanos-query-http:10902/
        isDefault: true
      - name: loki
        type: loki
        access: proxy
        url: http://loki.logs.svc.cluster.local:3100
      - name: influxdb
        type: influxdb
        access: proxy
        url: http://influxdb:8086
        database: telegraf
      - name: home_assistant
        type: influxdb
        access: proxy
        url: http://influxdb:8086
        database: home_assistant
      - name: speedtests
        type: influxdb
        access: proxy
        url: http://influxdb:8086
        database: speedtests
      - name: uptimerobot
        type: influxdb
        access: proxy
        url: http://influxdb:8086
        database: uptimerobot
      - name: TrueNAS
        type: influxdb
        access: proxy
        url: http://influxdb:8086
        database: graphitedb
      grafana.ini:
        paths:
          data: /var/lib/grafana/data
          logs: /var/log/grafana
          plugins: /var/lib/grafana/plugins
          provisioning: /etc/grafana/provisioning
        analytics:
          check_for_updates: true
        log:
          mode: console
        grafana_net:
          url: https://grafana.net
    kubeEtcd:
      enabled: false
    kubeControllerManager:
      enabled: false
      endpoints:
      - 10.20.10.16
    kubeScheduler:
      enabled: false
      endpoints:
      - 10.20.10.16
    kubeProxy:
      enabled: false
    prometheus-node-exporter:
      tolerations:
      - key: "arm"
        operator: "Exists"
      - key: "armhf"
        operator: "Exists"
    prometheus:
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: "nginx"
          kubernetes.io/tls-acme: "true"
          cert-manager.io/cluster-issuer: letsencrypt-prod
          # nginx.ingress.kubernetes.io/auth-url: "https://auth.holthome.net/oauth2/auth"
          # nginx.ingress.kubernetes.io/auth-signin: "https://auth.holthome.net/oauth2/start?rd=$escaped_request_uri"
        hosts:
        - prom-server.holthome.net
        tls:
        - hosts:
          - prom-server.holthome.net
          secretName: prom-cert
      prometheusSpec:
        # image:
        #   repository: quay.io/prometheus/prometheus
        #   tag: v2.20.0
        replicas: 2
        replicaExternalLabelName: "replica"
        ruleSelector: {}
        ruleNamespaceSelector: {}
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorSelector: {}
        serviceMonitorNamespaceSelector: {}
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelector: {}
        podMonitorNamespaceSelector: {}
        podMonitorSelectorNilUsesHelmValues: false
        retention: 6h
        enableAdminAPI: true
        walCompression: true
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: longhorn
              resources:
                requests:
                  storage: 10Gi
        thanos:
          image: quay.io/thanos/thanos:v0.17.2
          version: v0.17.2
          objectStorageConfig:
            name: thanos
            key: object-store.yaml
  valuesFrom:
  - kind: Secret
    name: "kube-prometheus-stack-helm-values"
    optional: false
